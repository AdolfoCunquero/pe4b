---
title: "Tema 3 - Variables Aleatorias discretas multidimensionales"
author: "Ricardo Alberich, Juan Gabriel Gomila y  Arnau Mir"
date: 
output: 
  ioslides_presentation: 
    css: Mery_style.css
    keep_md: no
    logo: Images/matriz_mov.gif
    widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Variables aleatorias bidimensionales discretas




## Dos variables aleatorias pe4b

RESUMIR 176-180 CON EJEMPLO INCLUIDO HASTA MEDIA Y VARIANZA CONDICIONAL


## Variables aleatorias bidimensionales discretas. Introducción

<l class="definition">Definición de variable aleatoria bidimensional discreta:</l>
Sea $(X,Y)$ una **variable aleatoria bidimensional**. Diremos que es discreta cuando su conjunto de valores en $\mathbb{R}^2$, $(X,Y)(\Omega)$ es un conjunto finito o numerable. 

En la mayoría de los casos, dicho conjunto será un subconjunto de los enteros naturales.

## Función de probabilidad conjunta
<l class="definition">Definición de función de probabilidad conjunta:</l>
Dada una **variable aleatoria bidimensional discreta** $(X,Y)$ con $(X,Y)(\Omega)=\{(x_i,y_j),\ i=1,2,\ldots,\ j=1,2,\ldots,\}$, definimos la función de probabilidad discreta $P_{XY}$ para un valor $(x,y)\in\mathbb{R}^2$ de la siguiente forma:
$$
\begin{array}{rl}
P_{XY}: \mathbb{R}^2 & \longrightarrow [0,1]\\
(x,y) & \longrightarrow P_{XY}(x,y)=P(X= x,\ Y= y).
\end{array}
$$


Llamaremos dominio de la variable conjunta a 
$$D_{XY}=\{(x,y)\in \mathbb{R}^2 | P_{XY}(x,y)=P(X= x,\ Y= y)>0\}.$$

Es decir es el conjunto de valores posibles que toma la v.a. $(X,Y)$.

## Función de probabilidad conjunta

Por tanto, de cara a calcular $P_{XY}$ basta calcular $P_{XY}(x_i,y_j)$ para $(x_i,y_j)\in D_{XY}$:

<div class="center">
| $X/Y$| $y_1$    | $y_2$  | $\ldots$ | $y_N$ |
|----|----|----|----|----|
| $x_1$| $P_{XY}(x_1,y_1)$ | $P_{XY}(x_1,y_2)$ | $\ldots$ | $P_{XY}(x_1,y_N)$|
| $x_2$| $P_{XY}(x_2,y_1)$ | $P_{XY}(x_2,y_2)$ | $\ldots$ | $P_{XY}(x_2,y_N)$|
| $\vdots$ |$\vdots$ |$\vdots$ |$\vdots$ |$\vdots$ |
| $x_M$| $P_{XY}(x_M,y_1)$ | $P_{XY}(x_M,y_2)$ | $\ldots$ | $P_{XY}(x_M,y_N)$|
</div>





## Propiedades de la función de probabilidad conjunta

Sea $(X,Y)$ una **variable aleatoria bidimensional discreta** con dominio $D_{XY}=\{(x_i,y_j)\, i=1,2,\ldots,\ j=1,2,\ldots\}$. Entonces su **función de probabilidad conjunta** verifica las propiedades siguientes:

La suma de todos los valores de la **función de probabilidad conjunta** sobre el conjunto de valores siempre vale 1: $$\sum_{i}\sum_j P_{XY}(x_i,y_j)=1.$$


## Propiedades de la función de probabilidad conjunta

Sea $B$ un subconjunto cualquiera del dominio $D_{XY}$. El valor de la probabilidad $P((X,Y)\in B)$ se puede calcular de la forma siguiente:
$$
P((X,Y)\in B) =\sum_{(x_i,y_j)\in B} P_{XY}(x_i,y_j).
$$
O sea, la probabilidad de que la variable bidimensional tome valores en $B$ es igual a la suma de todos aquellos valores de la función de probabilidad conjunta que están en $B$.

## Propiedades de la función de probabilidad conjunta

En particular, tenemos la relación siguiente que relaciona la **función de distribución conjunta** con la **función de probabilidad conjunta**:
$$
F_{XY}(x,y)=\sum_{x_i\leq x, y_j\leq y} P_{XY}(x_i,y_j).
$$
Dicha expresión se deduce de la expresión anterior considerando $B=(-\infty,x]\times (-\infty,y]$.



## Variables aleatorias marginales
Consideremos una variable aleatoria **bidimensional discreta $(X,Y)$** con **función de probabilidad conjunta** $P_{XY}(x_i,y_j)$, con $(x_i,y_j)\in D_{XY}$, $i=1,2,\ldots$, $j=1,2,\ldots$.

La tabla de la **función de probabilidad conjunta** contiene suficiente información para obtener las **funciones de probabilidad** de las variables $X$ e $Y$. 

Dichas variables $X$ e $Y$ se denominan **distribuciones marginales** y sus correspondientes **funciones de probabilidad**, **funciones de probabilidad marginales** $P_X$ de la variable $X$ y $P_Y$ de la variable $Y$.

Veamos cómo obtener $P_X$ y $P_Y$ a partir de la tabla $P_{XY}$.

## Variables aleatorias marginales
<l class="prop">Proposición. Expresión de las funciones de probabilidad marginales. </l>
Sea $(X,Y)$ una variable aleatoria **bidimensional discreta** con **función de probabilidad conjunta** $P_{XY}(x_i,y_j)$, con $(x_i,y_j)\in (X,Y)(\Omega)$, $i=1,2,\ldots$, $j=1,2,\ldots$.

Las **funciones de probabilidad marginales** $P_X(x_i)$ y $P_Y(y_j)$ se calculan usando las expresiones siguientes:
$$
\begin{array}{rl}
P_X(x_i)  & = \sum_{j=1} P_{XY}(x_i,y_j),\  i=1,2,\ldots,\\ P_Y(y_j) &  = \sum_{i=1} P_{XY}(x_i,y_j),\ \ j=1,2,\ldots
\end{array}
$$

## Variables aleatorias marginales


Podemos representar  $P_{XY}$ como una tabla bidimensional donde en la primera fila están los valores de la variable $Y$ ($y_1,y_2,\ldots$) y en la primera columna están los valores de la variable $X$ ($x_1,x_2,\ldots$), para obtener la **función de probabilidad marginal** de la variable $X$ en el valor $x_i$, $P_X(x_i)$, hay que sumar todos los valores de $P_{XY}(x_i,y_j)$ correspondientes a la fila $i$-ésima y para obtener la **función de probabilidad marginal** de la variable $Y$ en el valor $y_j$, $P_Y(y_j)$, hay que sumar todos los valores de $P_{XY}(x_i,y_j)$ correspondientes a la columna $j$-ésima.


##  Independencia de variables aleatorias discretas

Recordemos que dos sucesos $A$ y $B$ son independientes si $P(A\cap B)=P(A)\cdot P(B)$.

¿Cómo trasladar dicho concepto al caso de variables aleatorias?

En el caso de **variables aleatorias discretas bidimensionales** vimos que, dada una variable aleatoria bidimensional discreta $(X,Y)$ con $(X,Y)(\Omega)=\{(x_i,y_j),\ i=1,2,\ldots,j=1,2,\ldots\}$, los sucesos de la forma $\{X=x_i,\  Y=y_j\}$ determinaban cómo se distribuían los valores de la variable $(X,Y)$. De ahí la definición siguiente:

## Independencia de variables aleatorias discretas
<l class="definition">Definición de independencia para variables aleatorias bidimensionales discretas. </l>
Sean $(X,Y)$ una **variable aleatoria bidimensional discreta** con $(X,Y)(\Omega)=\{(x_i,y_j),\ i=1,2,\ldots,j=1,2,\ldots\}$ y **función de probabilidad** $P_{XY}$ y **funciones de probabilidad marginales** $P_X$ y $P_Y$. Entonces $X$ e $Y$ son independientes si:
$$
P_{XY}(x_i,y_j)=P_X(x_i)\cdot P_Y(y_j),\ i=1,2,\ldots,j=1,2,\ldots
$$
o dicho de otra forma:
$$
P(X=x_i,\ Y=y_k)=P(X=x_i)\cdot P(Y=y_j),\ i=1,2,\ldots,j=1,2,\ldots
$$




## Esperanza y varianza de las distribuciones  marginales


* $E(X)=\sum_{x\in D_X} x\cdot P(X=x).$
* $E(Y)=\sum_{x\in D_Y} y\cdot P(Y=y).$
* $Var(X)=E(X-E(X))=E(X)-E(X)^2.$
* $Var(Y)=E(Y-E(Y))=E(Y)-E(Y)^2$$

## Distibuciones condicionales


$$P(X=x|Y=y)=\frac{P_{XY}(x,y)}{P_Y(y)}=\frac{P(X=x,Y=y)}{P(Y=y)}$$
$$P(Y=y|X=x)=\frac{P_{XY}(x,y)}{P_X(x)}=\frac{P(X=x,Y=y)}{P(X=x)}$$

<l class="prop"> Propiedad <\l> Si las variables $X$ e $Y$ son independientes se cumple que $P(X=x|Y=y)=P(X=x)$ y que $P(Y=y|X=x)=P(Y=y)$.


## Esperanzas condicionales

$$E(X/Y=y)=\sum_{x\in D_X} x\cdot P(X=x/Y=y)$$

$$E(Y/X=x)=\sum_{y\in D_Y} y\cdot P(Y=y|X=x)$$

<l class="prop"> Propiedad <\l> Si las variables $X$ e $Y$ son independientes se cumple que $E(X/Y=y)=E(X)$ y que $E(Y/X=x)=E(Y)$






## Esperanzas de funciones de v.a. discretas bidimensionales


<l class="definition"> Definición: </l>

Sea $(X,Y)$ una variable aleatoria bidimensional  discreta y $g(X,Y)$ una función de esa variable bidimensional entonces $E(g(X,Y))=\sum_i\sum_j g(x_i,y_j) \cdot P(X=x_i,Y=y_j)$.

En particular:
 
 * $\scriptsize{\displaystyle E(X+Y)=\sum_i\sum_j (x_i+y_j) \cdot P(X=x_i,Y=y_j)}.$
 * $\scriptsize{\displaystyle Var(X+Y)=E(X+Y-E(X+Y))^2)=\sum_i\sum_j (x_i+y_j-(\mu_X+\mu_Y))\cdot P(X=x_i,Y=y_j).}$
 * $\scriptsize{\displaystyle Var(X+Y)=E(X+Y-E(X+Y))^2)=\sum_i\sum_j (x_i+y_j-(\mu_X+\mu_Y))\cdot P(X=x_i,Y=y_j).}$



## Esperanzas de funciones de v.a. discretas bidimensionales

<l class="prop"> Propiedad: </l>
Sea $(X,Y)$ una variable aleatoria bidimensional  entonces se cumple que:

 *  $E(X+Y)=E(X)+E(Y)=\mu_X\cdot \mu_y$
 *  Si   $X$ e $Y$ son independientes entonces  $E(X\cdot Y)=E(X)\cdot E(Y)=\mu_X\cdot \mu_y$
 *  Si   $X$ e $Y$ son independientes entonces  $Var(X+Y)=Var(X)\cdot Var(Y)=\mu_X\cdot \mu_y$
  




## Covarianza y correlación pe4b


RESUMIR FUSILAR PÁGINA 182. EJEMPLO DE LA PÁGINA 183 + RESUMEN 184 Y 185

## Medida de la variación conjunta: covarianza

El **momento conjunto centrado en las medias para $k=1$ y $l=1$** se denomina **covariancia** entre las variables $X$ e $Y$:
$$
\sigma_{XY}=Cov(X,Y)=E((X-\mu_X)(Y-\mu_Y)).
$$
La covarianza puede calcularse a partir de la **correlación** entre las variables:
$$
Cov(X,Y)=E((X-\mu_X)(Y-\mu_Y))=E(X\cdot Y)-\mu_X\cdot \mu_Y,
$$

<l class="prop">Propiedad. </l>
Si las variables $X$ e $Y$ son **independientes**, entonces $Cov(X,Y)=0$. 


Es una consecuencia de que si $X$ e $Y$ son independientes entonces que vimos que $E(X\cdot Y)=E(X)\cdot E(Y) =\mu_X\cdot \mu_y$.


## Covarianza entre las variables

La **covarianza** es una medida de lo relacionadas están las variables $X$ e $Y$:

* Si cuando $X\geq \mu_X$, también ocurre que $Y\geq \mu_Y$ o viceversa, cuando $X\leq \mu_X$, también ocurre que $Y\leq \mu_Y$, el valor $(X-\mu_X)(Y-\mu_Y)$ será positivo y la **covarianza** será positiva.

* Si por el contrario, cuando $X\geq \mu_X$, también ocurre que $Y\leq \mu_Y$ o viceversa, cuando $X\leq \mu_X$, también ocurre que $Y\geq \mu_Y$, el valor $(X-\mu_X)(Y-\mu_Y)$ será negativo y la **covarianza** será negativa.

* En cambio, si a veces ocurre una cosa y a veces ocurre otra, la **covarianza** va cambiando de signo y puede tener un valor cercano a 0.

## Propiedades de la covarianza
* Sea $(X,Y)$ una variable aleatoria bidimensional. Entonces la **varianza de la suma/resta** se calcula usando la expresión siguiente:
$$
Var(X\pm Y)=Var(X)+Var(Y)\pm 2\cdot Cov(X,Y).
$$

* Sea $(X,Y)$ una variable aleatoria bidimensional donde las variables $X$ e $Y$ son **independientes**. 
Entonces:
$$
Var(X+Y)=Var(X)+Var(Y).
$$



## Coeficiente de correlación

La **covarianza** depende de las unidades en las que están las variables $X$ e $Y$ ya que si $a>0$ y $b>0$, entonces:
$$
Cov(a\cdot X,b\cdot Y)=a\cdot b\cdot Cov(X,Y).
$$
Por tanto, si queremos "medir" la relación que existe entre las variables $X$ e $Y$ tendremos que "normalizar" la **covarianza** definiendo el **coeficiente de correlación** entre las variables $X$ e $Y$:

## Coeficiente de correlación entre las variables

<l class="definition">Definición del coeficiente de correlación. </l>
Sea $(X,Y)$ una variable aleatoria bidimensional. Se define el **coeficiente de correlación** entre las variables $X$ e $Y$ como: 
$$
\rho_{XY}=\frac{Cov(X,Y)}{\sqrt{Var(X)}\cdot\sqrt{Var(Y)}}=\frac{E(X\cdot Y)-\mu_X\cdot \mu_Y}{\sqrt{E\left(X^2\right)-\mu_X^2}\cdot \sqrt{E\left(Y^2\right)-\mu_Y^2}}.
$$


## Coeficiente de correlación entre las variables

<l class="observ">Observación. </l>
Si las variables $X$ e $Y$ son **independientes**, su **coeficiente de correlación** $\rho_{XY}=0$ es nulo ya que su **covarianza** lo es.

Notemos también que la **correlación** no tiene unidades y es invariante a cambios de escala.

Además, la **covarianza** de las **variables tipificadas** $\frac{X-\mu_X}{\sigma_X}$ y $\frac{Y-\mu_Y}{\sigma_Y}$ coincide con la **correlación** de $X$ e $Y$.

El **coeficiente de correlación** es un valor normalizado ya que siempre está entre -1 y 1: $-1\leq\rho_{XY}\leq 1$.


# Distribuciones multidimensionales

## Conceptos básicos

