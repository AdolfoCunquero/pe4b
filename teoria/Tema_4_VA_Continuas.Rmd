---
title: "Tema 2 - Variables Aleatorias continuas"
author: "Ricardo Alberich, Juan Gabriel Gomila y  Arnau Mir"
date: 
output: 
  ioslides_presentation: 
    css: Mery_style.css
    keep_md: no
    logo: Images/matriz_mov.gif
    widescreen: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Variables aleatorias continuas

## Variables aleatorias continuas definición.

* Las variables aleatorias **continuas**  toman  valores en intervalos.
 
* Lo más habitual es que estas variables tengan función de distribución continua y
derivable.

* En general, dado un valor $x_0$, $P(X=x_0)=0$ por lo que no nos es útil definir *función de probabilidad*.



## Variables aleatorias continuas

* En general tendremos que $P(X<x_0)=P(X\leq x_0)$.
* Por otra parte podemos utilizar una regla parecida del
cociente entre casos favorables y casos posibles de Laplace  pero en
este caso el conteo se hace por la *medida*  de los casos
posibles partida por la *medida* de los casos favorables.
* Veamos un ejemplo de v.a. continua, que ampliaremos en el tema siguiente, en el que se utilizan todos estos conceptos.




## Ejemplo: Distribución uniforme en $[0,1]$.

<div class="example"> 

**Ejemplo: distancia dardo centro de la diana** 

Supongamos que elegimos un número al azar en el intervalo $[0,1]$.

Consideremos la v.a. continua $X=$ número elegido.
</div>

<div class="example-sol">

Su función de distribución es 

$$
F_{X}(x)=
\left\{
\begin{array}{ll}
0, & \mbox{si } x\leq 0,\\
x, & \mbox{si } 0<x<1,\\
1, & \mbox{si } x\geq 1.
\end{array}
\right.
$$

consideremos

* C.F. *longitud favorable*  que es $x-0$,
* C.P. *longitud posible* que es $1-0$,

luego 

$$P(X\leq x)=\frac{C.F.}{C.P.}=\frac{x-0}{1-0}=x.$$


## Gráfica de la función de distribución uniforme

```{r figUNIF, fig.align='center',echo=FALSE}
curve(punif(x,0,1),xlim=c(-1,2),col="blue",
      main="Función de distribución de una v.a. uniforme en el intervalo unidad.")
```


## Función de densidad

<l class="definition"> Función de densidad </l> 

Una función $f:\mathbb{R}\to\mathbb{R}$ es una función de densidad sobre $\mathbb{R}$ si cumple que


* $f_{X}(x)\geq 0$ para todo $x \in\mathbb{R}.$
* $f$ es continua salvo a lo más en una cantidad finita de puntos sobre
cada intervalo acotado de $\mathbb{R}$.
* $\displaystyle\int\limits_{-\infty}^{+\infty} f_{X}(x) dx=1.$

## Función de distribución de una variable aleatoria.

<l class="definition"> **Función de distribución de una variable aleatoria** </l>

Sea $X$ una v.a. con función de distribución $F_X$. Sea $f:\mathbb{R}\to\mathbb{R}$ una función de densidad tal que

$$F_X(x)=\displaystyle\int_{-\infty}^{x} f_X(t) dt.\mbox{ para todo } x\in\mathbb{R},$$

Entonces $X$ es una variable aleatoria continua y $f_X$ es la densidad de la v.a. $X$.


##  Dominio de una variable aleatoria continua

El conjunto $D_X=\{x\in\mathbb{R}| f_x(x)>0\}$ recibe el nombre de <l class="definition"> soporte o dominio de la
variable aleatoria continua</l> y se interpreta como su conjunto de resultados posibles.


<div class="example">
**Ejemplo: diana  (continuación)**

En nuestra  ejemplo de la diana, la función $f$ es una densidad

$$
f_{X}(x)=\left\{
\begin{array}{ll}
0, & \mbox{si } x\leq 0,\\
1, & \mbox{si } 0 < x < 1,\\
0, & \mbox{si } 1\leq x.
\end{array}\right.
$$

que es la densidad de $X$, en efecto:

</div>

## Densidad diana

<div class="example-sol">

$$
f_{X}(x)=\left\{
\begin{array}{ll}
0, & \mbox{si } x\leq 0,\\
1, & \mbox{si } 0 < x < 1,\\
0, & \mbox{si } 1\leq x.
\end{array}\right.
$$

* Si $x \leq 0$ entonces $\displaystyle\int_{-\infty}^x f_X(t) dt = 0.$

* Si $0\leq x\leq 1$ entonces $\displaystyle\int_{-\infty}^x f_X(t) dt = \int_0^x 1 dt = x.$

* Si $x\geq 1$  entonces $\displaystyle\int_{-\infty}^x f_X(t) dt = \int_0^1 1 dt = 1.$


Por lo tanto,  $F_X(x)=\displaystyle\int_{-\infty}^x f_X(t) dt$ para todo $x\in\mathbb{R}.$

</div>

##  Densidad diana


<div class="example-sol">


```{r fig.align="center",  fig_caption="Función de densidad de una v.a. uniforme en el intervalo$(0,1)$"}
curve(dunif(x,0,1),xlim=c(-0.5,1.5),col="blue",
      main="Densidad de la distribución uniforme en [0,1]")
```
</div>

## Utilidad de la función de densidad

La función de densidad nos permite calcular diversas probabilidades.

<l class="prop">Propiedades de la función de densidad </l> 

* Sea $X$ una v.a. continua con función de distribución $F_X$ y de
densidad $f_X$, entonces
$$\begin{eqnarray*}
P(a< X< b) &=&  P(a<X\leq b)= P(a\leq X< b)=\\
 & & P(a\leq X\leq b)= \displaystyle\int_{a}^b f_X(x) dx.
\end{eqnarray*}
$$
* Si $A$ es un subconjunto  adecuado de $\mathbb{R}$ entonces 
$$P(X\in A)=\displaystyle\int_{A} f(x) dx=\displaystyle\int_{A\cap D_X} f(x) dx.
$$




## Utilidad de la función de densidad

<l class="prop">Propiedades de la función de densidad </l> 

Sea $X$ una v.a. continua con función de distribución $F_X$ y de densidad $f_X$, entonces:

* Si $f_x$ es continua en un punto $x$, $F_X$ es derivable en ese punto y
$F_X'(x)=f_X(x).$
* $P(X=x)=0$ para todo $x\in\mathbb{R}.$
  

<div  class="exercise"> **Ejercicio**

Comprobar estas propiedades en el ejemplo de la diana. </div>




## Ejemplo tiempo ejecución de un proceso

<div  class="example">
**Ejemplo: tiempo ejecución de un proceso.**

Sea $X=$ tiempo de ejecución de un proceso. Se supone que $X$ sigue una distribución uniforme en dos unidades de tiempo, si tarda más el proceso se cancela.

Calculemos la función de densidad y de distribución de la v.a $X$.

</div>

<div  class="example-sol">

Entonces

$$
F_{X}(x)=P(X\leq x)=\frac{CF}{CP}=\frac{x}2.
$$


Luego su función de distribución es:

$$
F_{X}(x)=\left\{\begin{array}{ll}
0, & \mbox{si } x\leq 0,\\
\frac{x}2 & \mbox{si } 0<x<2,\\
1, & \mbox{si } 2\leq x.
\end{array}\right.
$$

## Ejemplo tiempo ejecución de un proceso


Su función de densidad por su lado es:
$$
f_{X}(x)=F_{X}'(x)=\left\{\begin{array}{ll}
0 & \mbox{si } x\leq 0\\
\frac12 & \mbox{si } 0<x\leq 2\\
0 & \mbox{si } 2\leq x
\end{array}\right.
$$

Efectivamente

* $f_{X}(x)\geq 0,$ y tiene un conjunto finito de discontinuidades: en $0$ y en $2$
* $F_X(x)=\displaystyle\int_{-\infty}^x f_X(t) dt,$ para todo $x\in \mathbb{R}$ <l class="exercise">(Ejercicio: 
resolverlo gráficamente.)</l>
* $\displaystyle\int_{-\infty}^{+\infty}f_{X}(x)dx=
\int_0^2\frac12dx=\left[\frac{x}2\right]_0^2
=\frac22-\frac02=1.$

</div>

## Ejemplo tiempo ejecución de un proceso

<div  class="exercise">
**Ejercicio: Tiempo de un proceso:**

Calcular la probabilidad de que uno de nuestros procesos tarde
más de una unidad de tiempo en ser procesado. Calcular también la  probabilidad de
que dure entre $0.5$ y $1.5$ unidades de tiempo.
</div>

# Esperanza y varianza para variables aleatorias continuas

## Esperanza y varianza para variables aleatorias continuas

Los mismos comentarios y definiciones que se dieron en la sección correspondiente del tema
de estadística descriptiva son aplicables aquí.

Así que sólo daremos las definiciones, la forma de cálculo y algunos ejemplos.

En lo que sigue, salvo que diagamos lo contrario,  $X$  es una v.a. continua con función de densidad $f_{X}(x)$



## Esperanza y varianza para variables aleatorias continuas

<l class="definition">Esperanza v.a. continuas </l>

* Su esperanza es:
$$E(X)=\displaystyle\int\limits_{-\infty}^{+\infty} x\cdot f_{X}(x)dx.$$
* Si $g(x)$ es una función de la variable $X$ entonces:
$$E(g(X))=\displaystyle\int\limits_{-\infty}^{+\infty} g(x)\cdot f_{X}(x)dx.$$

## Esperanza y varianza para variables aleatorias continuas

<l class="definition"> Varianza v.a. continuas </l>


* Su varianza es:
$$
Var(X)=\sigma_{X}^2=E((X-\mu_{X})^2)=
\displaystyle\int\limits_{-\infty}^{+\infty} (x-\mu_{X})^2 \cdot f_{X}(x)dx.
$$
* Su desviación típica es:  $$\sigma_{X}=+\sqrt{\sigma_{X}^2}.$$


## Propiedades

<l class="prop"> Propiedades </l>

* $\sigma_{X}^2\geq 0$.
* $Var(cte)=E(cte^2)-(E(cte))^2= cte^2 - cte^2=0$.
* $\displaystyle Var(x)=E(X^2)-\mu_{X}^2=\int_{-\infty}^{+\infty}x^2\cdot  f_{X}(x)dx - \mu_{X}^2.$
* El mínimo de $E((X-C)^2)$ se alcanza cuando $C=E(X)$ y es $Var(X)$.



## Ejemplo

<div  class="example">

**Ejemplo: diana (continuación)**

Calcular $\mu_{X}$ y $\sigma_{X}^2$ en  el ejemplo de la diana
</div>

<div  class="example-sol">
Resultado 
$$\mu_{X}=\frac12,$$ 
$$E(X^2)=\frac13,$$
$$Var(X)=\frac1{12}.$$

</div>


## Esperanza de trans. lineales de v.a. continuas

<l class="prop">Proposición</l>

Sea $X$ una v.a. continua con $E(X)=\mu_{X}$ y $Var(X)=\sigma_{X}^2$ sea $Y=a+b X$, donde
$a,b\in\mathbb{R}$, es una nueva v.a. continua obtenida mediante una transformación lineal de $X$.
Se verifican las mismas propiedades que en el caso discreto:

* $E(Y)=E(a+b\cdot  X)=a+b\cdot  E(X)$.
* $Var(Y)=Var(a+b\cdot  X)=b^2 \cdot  Var(X)$.
* $\sigma_{Y}=|b|\cdot  \sigma_{X}$.
* $Z=\frac{X-\mu_{X}}{\sigma_{X}}$ es una transformación
lineal de $X$ de forma que $$E(Z)=0 \mbox{ y } Var(Z)=1$$





## Ejemplo

<div class="example"> 
**Ejemplo**

En una empresa de venta de vinos por internet, sea
$X=$ número de  litros de vino del país vendidos en un año.
Supongamos que sabemos que $E(X)=10000$  y que $Var(X)=100.$
Supongamos que los gastos fijos de distribución son
50.000 &euro; y el beneficio por litro es de 10 &euro; por botella.
Definimos $T=10\cdot X-50000,$ que será el beneficio después de gastos.
</div>

<div class="example-sol"> 

Entonces la esperanza del beneficio es 
$$E(T)=10 E(X)-50000 = 50000,$$
y
$$Var(T)=10^2 Var(X)= 10000.$$
</div>

# Transformaciones de variables aleatorias


## Transformaciones de variables aleatorias

Muchas variables aleatorias son funciones de otras v.a. En lo que sigue resumiremos diversas técnicas para dada una v.a. $X$ y una
transformación $Y=h(X)$  encontrar $F_{Y}$ a
partir de $F_{X}$.


## Propiedad

<l class="prop">Transformaciones de v.a. discretas </l>

Sea $X$ una v.a. discreta con $X(\Omega)=\{x_1,x_2,\ldots,x_{n},..\}$ y sea $h:\mathbb{R}\to\mathbb{R}$ una aplicación.
Entonces $Y=h(X)$ es también una v.a. discreta. Además si $P_X$
y $F_{X}$ son las funciones de probabilidad y de distribución de
$X$ entonces

 
* $\displaystyle P_{Y}(y)=\sum_{x_{i}|h(x_{i})=y}P_X(x_{i}).$
* $\displaystyle F_{Y}(y)=\sum_{x_{i}|h(x_{i})\leq y} P_X(x_{i}).$

## Propiedades

Desafortunadamente para variables no discretas el resultado  no es tan sencillo como el anterior, pues la transformación de, por ejemplo, una v.a. continua puede ser continua, discreta, mixta,$\ldots$

<l class="prop">Transformación de v.a. continuas en continuas</l>

Sea $X$ una v.a. continua cuya función de densidad es $f_{X}$. Sea
$h:\mathbb{R}\to\mathbb{R}$, una aplicación estrictamente monótona y derivable, por lo tanto
$h'(x)\not=0$ para todo $x\in\mathbb{R}$. Sea $Y=h(X)$ la
transformación de $X$ por $h$. Entonces $Y$ es una v.a. continua con función
de densidad

$$f_{Y}(y)=\left.\frac{f_{X}(x)}
{\left|h'(x)\right|}\right|_{x=h^{-1}(y)}$$



## Propiedades


<l class="prop">Densidad de una transformación de una v.a. continua</l>

Sea $X$ una v.a. continua cuya función de densidad es $f_{X}$.  Sea 
$$h:\mathbb{R}\to\mathbb{R}$$ 
una aplicación, no necesariamente monótona tal que :

* sea derivable con derivada no nula
* la ecuación $h(x)=y$ tiene un número finito de soluciones
$x_1,x_2,..,x_{n}$

entonces:

$$
\displaystyle f_{Y}(y)=\left.\sum_{k=1}^{n} \frac{f_{X}(x)}
{\left|h'(x)\right|}\right|_{x=x_{k}}.
$$


## Método general de transformación de v.a.

Cuando no podamos aplicar las propiedades anteriores intentaremos
calcular primero la función de distribución de la transformación
y luego su densidad.

Notemos que en general si $Y=g(X)$ es una v.a. transformación  de la
v.a. $X$ entonces

$$
F_{Y}(y)=P(Y\leq y)=P(g(X)\leq y).
$$

## Método general del transformación de variables aleatorias


Por ejemplo, si $g$ es estrictamente creciente y continua,

$$
F_{Y}(y)=P(g(X)\leq y)=P(X\leq g^{-1}(y))=F_{X}(g^{-1}(y)),
$$

y si $g$ es estrictamente decreciente y continua,
$$
F_{Y}(y)=P(g(X)\leq y)=P(X\geq g^{-1}(y))=1-F_{X}(g^{-1}(y)).
$$



# Desigualdades   básicas: Markov y Chebychev



## Desigualdades de Markov y de Chebychev

* En esta sección distintas desigualdades que acotan determinadas probabilidades de
una variable aleatoria.
* Estas desigualdades sirven en algunos casos para acotar probabilidades de determinados sucesos.
* También son útiles desde el punto de vista teórico, por ejemplo para justificar que la varianza es una medida de la dispersión de los datos.




## Desigualdad de Markov

<l class="prop"> Desigualdad de Markov</l>

Sea $X$ una v.a. positiva con $E(X)$ finita. Entonces 

$$P(X\geq a)\leq \frac{E(X)}{a}\mbox{ para todo }a>0.$$

<div class="dem">
**Demostración**:

Si $X$ es continua  y solo toma valores positivos

$$
\begin{eqnarray*}
E(X) &=& \int_{-\infty}^{+\infty} x\cdot f_{X}(x) dx=  \int_0^{+\infty} x\cdot f_{X}(x) dx=  \int_0^{a} x\cdot f_{X}(x) dx +\int_{a}^{+\infty} x\cdot f_{X}(x) dx \\
& &\geq   \int_{a}^{+\infty} x\cdot
f_{X}(x) dx \geq a \int_{a}^{+\infty}
f_{X}(x) dx = a \cdot  P(X\geq a),
\end{eqnarray*}
$$

de donde se sigue que 

$$P(X\geq a)\leq \frac{E(X)}{a}.$$

</div> 

## Desigualdad  de Markov

<l class="prop">Corolario</l>

Sea $X$ una v.a. con $E(X)$ finita entonces  para todo $a>0$

$$P(|X|\geq a )\leq \frac{E(|X|)}{a}.$$
<div class="exercise">
**Ejercicio**

Demuestra el corolario anterior a partir de la desigualdad de Markov.
</div>

## Desigualdad de Chebychev


La **desigualdad de Chebychev** también se denomina de Chebyshov y en inglés *Chebyshev*.

<l class="prop"> Desigualdad de Chebychev</l>

Sea  $X$ una v.a.con $E(X)=\mu$ y $Var(X)=\sigma^2$  entonces para todo $a>0$,

$$P(|X-\mu|\geq a)\leq \frac{\sigma^2}{a^2}.$$ 

## Demostración desigualdad de Chebychev

<div class="dem"> **Demostración**

Apliquemos la consecuencia de la desigualdad de Markov a la v.a.
no negativa 

$$Y^2=(X-\mu)^2$$

entonces

$$
P(Y^2\geq a^2) \leq 
\frac{E(Y^2)}{a^2}=\frac{E((X-\mu)^2)}{a^2}
= \frac{Var(X)}{a^2}=\frac{\sigma^2}{a^2}
.
$$




Por otra parte

$$
P(Y^2\geq a^2)=P(|Y|\geq a)= P(|X-\mu|\geq a),
$$

hecho que, junto con la desigualdad anterior,
demuestra el resultado.

</div>


## Uso de la desigualdad de Chebychev

<div class="observ"> **Utilidad básica de la desigualdad de Chebychev**</div>

Supongamos que $X$ es una v.a. con $Var(X)=0$, entonces, aplicando la desigualdad anterior

$$P(|X-E(X)|\geq a )=0\mbox{ para todo }a>0,$$ 

lo que implica que

$$P(X=E(X))=1,$$

Por lo que  la probabilidad de que $X$ sea constantemente $E(X)$ es 1,  hecho que nos  confirma la utilidad de la varianza como una medida de la dispersión de los datos.



## Ejemplo

<div class="example">
**Ejemplo:  tiempo de respuesta**

Se sabe que el tiempo de respuesta medio y la desviación típica de un sistema multiusuario  son 15 y 3 unidades de tiempo respectivamente. Entonces:

$$
P(|X-15|\geq 5)\leq \frac9{25}=0.36.
$$

</div>

Si substituimos $a$ por $a\cdot \sigma$ en la
desigualdad de Chebychev, nos queda:

$$
P(|X-\mu|\geq a\cdot \sigma)\leq
\frac{\sigma^2}{(a\cdot \sigma)^2}=\frac1{a^2},
$$

que es otra manera de expresar la desigualdad de Chebychev.


## Más formas de la desgualdad de Chebychev

La desigualdad de Chebychev también se puede escribir de al menos dos maneras más:

$$
P(\mu-a\leq X\leq \mu+a)\geq 1-\frac{\sigma^2}{a^2},
$$


y tomado como $a=k\cdot \sigma$,

$$
P(\mu-k\cdot \sigma\leq X\leq \mu+ k \cdot \sigma)\geq 1-\frac1{k^2}.
$$


## La varianza como medida de dispersión

Tomando la segunda expresión que hemos visto para la desigualdad de
Chebychev para distintos valores de $k>0$ obtenemos la siguiente tabla:


| k | $P(|X-E(X)|\geq k  \cdot \sigma)$|
|---|---|
| 1 | $\leq 1$ |
| 2 | $\leq 0.25$ |
| 3 | $\leq 0.111$ |
| 4 | $\leq 0.0025$ |

## Interpretación de la desigualdad

* Por ejemplo para $k=2$, esta desigualdad se puede interpretar como que, dada una v.a. $X$ con cualquier distribución que tenga $E(X)$ y $Var(X)$ finitos, *la  probabilidad de que un valor se aleje de la media $\mu$ más de $a=2$ desviaciones típicas es menor o igual que $0.25$*.

* Es decir sólo el 25\% de los valores estarán alejados de la media
más de $2\cdot \sigma$

¡*Sea cual sea la distribución de la v.a.*!




